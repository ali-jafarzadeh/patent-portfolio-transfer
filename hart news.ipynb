{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7708c702-7030-4d8b-aae5-9a345d70f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== STEP 0: ENV CHECK ==========\n",
      "CWD: /Users/alijafarzadeh/Desktop/Hart/codes\n",
      "INFILE: deals_for_sec.csv\n",
      "OUTDIR: /Users/alijafarzadeh/Desktop/Hart/codes/newsdata\n",
      "Will write: newsdata/news_queries.csv newsdata/raw_news_hits.csv newsdata/news_hits_deduped.csv newsdata/news_price_evidence_template.csv\n",
      "=======================================\n",
      "\n",
      "========== STEP 1: LOAD DEALS + BUILD QUERIES ==========\n",
      "[info] deals loaded: (1337, 10)\n",
      "[info] deal_date parse failures: 0\n",
      "[OK] wrote queries -> newsdata/news_queries.csv | rows=5,348\n",
      "[info] example queries:\n",
      "   deal_id             query_type  \\\n",
      "0        0      buyer_patents_acq   \n",
      "1        0    buyer_acquired_from   \n",
      "2        0  seller_portfolio_sale   \n",
      "3        0      buyer_ip_purchase   \n",
      "4        1      buyer_patents_acq   \n",
      "\n",
      "                                          query_text  date_start    date_end  \n",
      "0        AMAZON TECHNOLOGIES INC patents acquisition  2020-02-22  2020-06-21  \n",
      "1  AMAZON TECHNOLOGIES INC acquired patents from ...  2020-02-22  2020-06-21  \n",
      "2                    A9COM INC patent portfolio sale  2020-02-22  2020-06-21  \n",
      "3  AMAZON TECHNOLOGIES INC \"intellectual property...  2020-02-22  2020-06-21  \n",
      "4       ABBOTT DIABETES CARE INC patents acquisition  2014-06-19  2014-10-17  \n",
      "[time] step1: 0.7s\n",
      "========================================================\n",
      "\n",
      "========== STEP 2: DOWNLOAD NEWS HITS (RSS) ==========\n",
      "[info] RSS approach: query + after:YYYY-MM-DD before:YYYY-MM-DD (embedded in q)\n",
      "[info] checkpoint every 50 queries; sleep 0.2 sec each request\n",
      "[info] raw hits file: newsdata/raw_news_hits.csv\n",
      "[resume] loaded existing raw hits: (282, 15)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a0867490c1455e8369a2c7971dd223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RSS queries:   0%|          | 0/5348 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 199\u001b[0m\n\u001b[1;32m    196\u001b[0m url \u001b[38;5;241m=\u001b[39m _rss_url(q)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTIMEOUT_S\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    201\u001b[0m     items \u001b[38;5;241m=\u001b[39m _parse_rss(r\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    729\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:404\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1060\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1060\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n\u001b[1;32m   1063\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1064\u001b[0m         (\n\u001b[1;32m   1065\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnverified HTTPS request is being made to host \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1071\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connection.py:419\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_certs\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(context, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_default_certs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    416\u001b[0m ):\n\u001b[1;32m    417\u001b[0m     context\u001b[38;5;241m.\u001b[39mload_default_certs()\n\u001b[0;32m--> 419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# for the host.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    436\u001b[0m     default_ssl_context\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock\u001b[38;5;241m.\u001b[39mversion() \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTLSv1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTLSv1.1\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    440\u001b[0m ):  \u001b[38;5;66;03m# Defensive:\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/util/ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    437\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn HTTPS request has been made, but the SNI (Server Name \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndication) extension to TLS is not available on this platform. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m         SNIMissingWarning,\n\u001b[1;32m    446\u001b[0m     )\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m send_sni:\n\u001b[0;32m--> 449\u001b[0m     ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     ssl_sock \u001b[38;5;241m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/util/ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m server_hostname:\n\u001b[0;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(sock)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:455\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    450\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    451\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    452\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1046\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   1044\u001b[0m             \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1046\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1048\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1321\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1321\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NEWS ROUTE (Google News, FREE via RSS) — ONE SCRIPT\n",
    "==================================================\n",
    "What this does (end-to-end):\n",
    "1) Load 1,337 deals from deals_for_sec.csv from USPTO transfers\n",
    "2) Build deal-specific news queries (±60 days)\n",
    "3) Pull Google News RSS results for each query (checkpoint save so you never lose progress)\n",
    "4) Parse dates + enforce window\n",
    "5) De-duplicate within deal\n",
    "6) Create manual evidence template CSV\n",
    "7) Print diagnostics throughout so you see what’s going on\n",
    "\"\"\"\n",
    "import pandas as pd \n",
    "import os, re, time, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "from urllib.parse import quote_plus\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -----------------------\n",
    "# CONFIG (EDIT IF NEEDED)\n",
    "# -----------------------\n",
    "INFILE = \"deals_for_sec.csv\"\n",
    "\n",
    "OUTDIR = Path(\"newsdata\")\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "QUERIES_OUT = OUTDIR / \"news_queries.csv\"\n",
    "RAW_HITS_OUT = OUTDIR / \"raw_news_hits.csv\"\n",
    "DEDUP_OUT = OUTDIR / \"news_hits_deduped.csv\"\n",
    "EVIDENCE_TEMPLATE_OUT = OUTDIR / \"news_price_evidence_template.csv\"\n",
    "\n",
    "WINDOW_DAYS = 60\n",
    "QUERY_VERSION = \"v1\"\n",
    "\n",
    "# RSS pacing (avoid hammering)\n",
    "SLEEP_S = 0.2\n",
    "TIMEOUT_S = 20\n",
    "CHECKPOINT_EVERY = 50  # save every N queries\n",
    "\n",
    "# limit RSS results? RSS usually returns top items; leave as-is.\n",
    "\n",
    "# -----------------------\n",
    "# REQUIRED COLUMNS\n",
    "# -----------------------\n",
    "REQ_COLS = [\"deal_id\",\"or_name\",\"ee_name\",\"deal_date\",\"n_patents\",\"seller_cik\",\"buyer_cik\"]\n",
    "\n",
    "# -----------------------\n",
    "# HELPERS\n",
    "# -----------------------\n",
    "def _to_dt(x):\n",
    "    return pd.to_datetime(x, errors=\"coerce\")\n",
    "\n",
    "def _norm_firm(s: str) -> str:\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"[^\\w\\s&.-]\", \" \", s)\n",
    "    s = re.sub(r\"\\b(inc|inc\\.|corp|corp\\.|co|co\\.|ltd|ltd\\.|llc|plc|sa|s\\.a\\.|ag|gmbh|bv|sarl|pte|holdings?)\\b\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _hash16(x: str) -> str:\n",
    "    return hashlib.sha256(x.encode(\"utf-8\", errors=\"ignore\")).hexdigest()[:16]\n",
    "\n",
    "def _rss_url(q: str) -> str:\n",
    "    base = \"https://news.google.com/rss/search\"\n",
    "    params = f\"q={quote_plus(q)}&hl=en-US&gl=US&ceid=US:en\"\n",
    "    return f\"{base}?{params}\"\n",
    "\n",
    "def _parse_rss(xml_text: str):\n",
    "    \"\"\"\n",
    "    Returns list of dicts with title, description, source, pubDate, link.\n",
    "    \"\"\"\n",
    "    root = ET.fromstring(xml_text)\n",
    "    out = []\n",
    "    for item in root.findall(\".//item\"):\n",
    "        title = (item.findtext(\"title\") or \"\").strip()\n",
    "        link = (item.findtext(\"link\") or \"\").strip()\n",
    "        pubDate = (item.findtext(\"pubDate\") or \"\").strip()\n",
    "        source_el = item.find(\"source\")\n",
    "        source = (source_el.text or \"\").strip() if source_el is not None else \"\"\n",
    "        desc = (item.findtext(\"description\") or \"\").strip()\n",
    "        out.append({\"title\": title, \"desc\": desc, \"source\": source, \"pubDate\": pubDate, \"link\": link})\n",
    "    return out\n",
    "\n",
    "# Stable schema so empty files don’t crash later\n",
    "HIT_COLS = [\n",
    "    \"deal_id\",\"query_type\",\"query_text\",\"date_start\",\"date_end\",\n",
    "    \"buyer_name\",\"seller_name\",\"seller_cik\",\"buyer_cik\",\"n_patents\",\n",
    "    \"article_title\",\"article_snippet\",\"news_source\",\"publication_date_raw\",\"article_url\"\n",
    "]\n",
    "\n",
    "# -----------------------\n",
    "# STEP 0 — ENV CHECK\n",
    "# -----------------------\n",
    "print(\"========== STEP 0: ENV CHECK ==========\")\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"INFILE:\", INFILE)\n",
    "print(\"OUTDIR:\", OUTDIR.resolve())\n",
    "print(\"Will write:\", QUERIES_OUT, RAW_HITS_OUT, DEDUP_OUT, EVIDENCE_TEMPLATE_OUT)\n",
    "print(\"=======================================\\n\")\n",
    "\n",
    "# -----------------------\n",
    "# STEP 1 — LOAD DEALS + BUILD QUERIES\n",
    "# -----------------------\n",
    "print(\"========== STEP 1: LOAD DEALS + BUILD QUERIES ==========\")\n",
    "t0 = time.time()\n",
    "\n",
    "deals = pd.read_csv(INFILE)\n",
    "print(\"[info] deals loaded:\", deals.shape)\n",
    "\n",
    "missing = [c for c in REQ_COLS if c not in deals.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns in {INFILE}: {missing}\")\n",
    "\n",
    "deals[\"deal_date\"] = _to_dt(deals[\"deal_date\"])\n",
    "bad_n = int(deals[\"deal_date\"].isna().sum())\n",
    "print(\"[info] deal_date parse failures:\", bad_n)\n",
    "if bad_n > 0:\n",
    "    print(deals.loc[deals[\"deal_date\"].isna(), [\"deal_id\",\"deal_date\"]].head(10))\n",
    "    raise ValueError(\"Fix deal_date parsing before continuing.\")\n",
    "\n",
    "qtpls = {\n",
    "    \"buyer_patents_acq\":       '{buyer} patents acquisition',\n",
    "    \"buyer_acquired_from\":     '{buyer} acquired patents from {seller}',\n",
    "    \"seller_portfolio_sale\":   '{seller} patent portfolio sale',\n",
    "    \"buyer_ip_purchase\":       '{buyer} \"intellectual property\" purchase',\n",
    "}\n",
    "\n",
    "qrows = []\n",
    "for _, r in deals.iterrows():\n",
    "    buyer = str(r[\"ee_name\"])\n",
    "    seller = str(r[\"or_name\"])\n",
    "    d0 = r[\"deal_date\"] - timedelta(days=WINDOW_DAYS)\n",
    "    d1 = r[\"deal_date\"] + timedelta(days=WINDOW_DAYS)\n",
    "\n",
    "    for qtype, tpl in qtpls.items():\n",
    "        qrows.append({\n",
    "            \"deal_id\": r[\"deal_id\"],\n",
    "            \"buyer_name\": buyer,\n",
    "            \"seller_name\": seller,\n",
    "            \"buyer_name_norm\": _norm_firm(buyer),\n",
    "            \"seller_name_norm\": _norm_firm(seller),\n",
    "            \"deal_date\": r[\"deal_date\"].date().isoformat(),\n",
    "            \"date_start\": d0.date().isoformat(),\n",
    "            \"date_end\": d1.date().isoformat(),\n",
    "            \"query_type\": qtype,\n",
    "            \"query_text\": tpl.format(buyer=buyer, seller=seller),\n",
    "            \"query_version\": QUERY_VERSION,\n",
    "            \"seller_cik\": r[\"seller_cik\"],\n",
    "            \"buyer_cik\": r[\"buyer_cik\"],\n",
    "            \"n_patents\": r[\"n_patents\"],\n",
    "        })\n",
    "\n",
    "news_queries = pd.DataFrame(qrows)\n",
    "news_queries.to_csv(QUERIES_OUT, index=False)\n",
    "\n",
    "print(f\"[OK] wrote queries -> {QUERIES_OUT} | rows={len(news_queries):,}\")\n",
    "print(\"[info] example queries:\")\n",
    "print(news_queries[[\"deal_id\",\"query_type\",\"query_text\",\"date_start\",\"date_end\"]].head(5))\n",
    "print(f\"[time] step1: {time.time()-t0:.1f}s\")\n",
    "print(\"========================================================\\n\")\n",
    "\n",
    "# -----------------------\n",
    "# STEP 2 — DOWNLOAD NEWS HITS (RSS) WITH CHECKPOINTS\n",
    "# -----------------------\n",
    "print(\"========== STEP 2: DOWNLOAD NEWS HITS (RSS) ==========\")\n",
    "print(\"[info] RSS approach: query + after:YYYY-MM-DD before:YYYY-MM-DD (embedded in q)\")\n",
    "print(\"[info] checkpoint every\", CHECKPOINT_EVERY, \"queries; sleep\", SLEEP_S, \"sec each request\")\n",
    "print(\"[info] raw hits file:\", RAW_HITS_OUT)\n",
    "\n",
    "# resume if exists\n",
    "if RAW_HITS_OUT.exists():\n",
    "    raw_hits = pd.read_csv(RAW_HITS_OUT)\n",
    "    # enforce schema\n",
    "    for c in HIT_COLS:\n",
    "        if c not in raw_hits.columns:\n",
    "            raw_hits[c] = pd.NA\n",
    "    raw_hits = raw_hits[HIT_COLS]\n",
    "    print(\"[resume] loaded existing raw hits:\", raw_hits.shape)\n",
    "else:\n",
    "    raw_hits = pd.DataFrame(columns=HIT_COLS)\n",
    "    raw_hits.to_csv(RAW_HITS_OUT, index=False)  # create file immediately\n",
    "    print(\"[init] created empty raw hits file\")\n",
    "\n",
    "t1 = time.time()\n",
    "new_rows = []\n",
    "nonempty_queries = 0\n",
    "errors = 0\n",
    "\n",
    "for idx, row in tqdm(news_queries.iterrows(), total=len(news_queries), desc=\"RSS queries\"):\n",
    "    # embed date window operators into the query\n",
    "    q = f'{row[\"query_text\"]} after:{row[\"date_start\"]} before:{row[\"date_end\"]}'\n",
    "    url = _rss_url(q)\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, timeout=TIMEOUT_S)\n",
    "        r.raise_for_status()\n",
    "        items = _parse_rss(r.text)\n",
    "    except Exception:\n",
    "        items = []\n",
    "        errors += 1\n",
    "\n",
    "    if len(items) > 0:\n",
    "        nonempty_queries += 1\n",
    "\n",
    "    for it in items:\n",
    "        new_rows.append({\n",
    "            \"deal_id\": row[\"deal_id\"],\n",
    "            \"query_type\": row[\"query_type\"],\n",
    "            \"query_text\": q,\n",
    "            \"date_start\": row[\"date_start\"],\n",
    "            \"date_end\": row[\"date_end\"],\n",
    "            \"buyer_name\": row[\"buyer_name\"],\n",
    "            \"seller_name\": row[\"seller_name\"],\n",
    "            \"seller_cik\": row[\"seller_cik\"],\n",
    "            \"buyer_cik\": row[\"buyer_cik\"],\n",
    "            \"n_patents\": row[\"n_patents\"],\n",
    "            \"article_title\": it[\"title\"],\n",
    "            \"article_snippet\": it[\"desc\"],\n",
    "            \"news_source\": it[\"source\"],\n",
    "            \"publication_date_raw\": it[\"pubDate\"],\n",
    "            \"article_url\": it[\"link\"],\n",
    "        })\n",
    "\n",
    "    # checkpoint save\n",
    "    if (idx + 1) % CHECKPOINT_EVERY == 0:\n",
    "        if len(new_rows) > 0:\n",
    "            raw_hits = pd.concat([raw_hits, pd.DataFrame(new_rows, columns=HIT_COLS)], ignore_index=True)\n",
    "            new_rows = []\n",
    "        raw_hits.to_csv(RAW_HITS_OUT, index=False)\n",
    "\n",
    "        elapsed = time.time() - t1\n",
    "        print(f\"[checkpoint] {idx+1:,}/{len(news_queries):,} | total_hits={len(raw_hits):,} | nonempty_queries={nonempty_queries:,} | errors={errors:,} | elapsed={elapsed:.1f}s\")\n",
    "\n",
    "    if SLEEP_S and SLEEP_S > 0:\n",
    "        time.sleep(SLEEP_S)\n",
    "\n",
    "# flush remainder\n",
    "if len(new_rows) > 0:\n",
    "    raw_hits = pd.concat([raw_hits, pd.DataFrame(new_rows, columns=HIT_COLS)], ignore_index=True)\n",
    "raw_hits.to_csv(RAW_HITS_OUT, index=False)\n",
    "\n",
    "print(f\"[OK] finished RSS download. raw_hits rows={len(raw_hits):,}\")\n",
    "print(f\"[diag] queries with >=1 item: {nonempty_queries:,} / {len(news_queries):,}\")\n",
    "print(f\"[diag] request errors: {errors:,}\")\n",
    "print(f\"[time] step2: {time.time()-t1:.1f}s\")\n",
    "print(\"======================================================\\n\")\n",
    "\n",
    "# -----------------------\n",
    "# STEP 3 — PARSE DATES + ENFORCE WINDOW (IDEMPOTENT)\n",
    "# -----------------------\n",
    "print(\"========== STEP 3: PARSE DATES + WINDOW FILTER ==========\")\n",
    "t3 = time.time()\n",
    "\n",
    "raw_hits = pd.read_csv(RAW_HITS_OUT)\n",
    "print(\"[info] loaded raw_hits:\", raw_hits.shape)\n",
    "\n",
    "if len(raw_hits) == 0:\n",
    "    print(\"[warn] raw_hits is empty. Stopping after creating files.\")\n",
    "else:\n",
    "    raw_hits[\"publication_date\"] = pd.to_datetime(raw_hits[\"publication_date_raw\"], errors=\"coerce\", utc=True)\n",
    "    raw_hits[\"date_start_dt\"] = pd.to_datetime(raw_hits[\"date_start\"], errors=\"coerce\", utc=True)\n",
    "    raw_hits[\"date_end_dt\"] = pd.to_datetime(raw_hits[\"date_end\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "    na_pub = float(raw_hits[\"publication_date\"].isna().mean())\n",
    "    print(\"[diag] publication_date NA share:\", na_pub)\n",
    "\n",
    "    in_window = raw_hits[\"publication_date\"].isna() | (\n",
    "        (raw_hits[\"publication_date\"] >= raw_hits[\"date_start_dt\"]) &\n",
    "        (raw_hits[\"publication_date\"] <= raw_hits[\"date_end_dt\"])\n",
    "    )\n",
    "    before = len(raw_hits)\n",
    "    raw_hits = raw_hits.loc[in_window].copy()\n",
    "    after = len(raw_hits)\n",
    "\n",
    "    raw_hits.to_csv(RAW_HITS_OUT, index=False)\n",
    "    print(f\"[OK] window filter: {before:,} -> {after:,} rows\")\n",
    "    print(f\"[time] step3: {time.time()-t3:.1f}s\")\n",
    "\n",
    "print(\"========================================================\\n\")\n",
    "\n",
    "# -----------------------\n",
    "# STEP 4 — DE-DUP WITHIN DEAL (IDEMPOTENT)\n",
    "# -----------------------\n",
    "print(\"========== STEP 4: DEDUP WITHIN DEAL ==========\")\n",
    "t4 = time.time()\n",
    "\n",
    "raw_hits = pd.read_csv(RAW_HITS_OUT)\n",
    "print(\"[info] loaded windowed raw_hits:\", raw_hits.shape)\n",
    "\n",
    "def _fp(row):\n",
    "    t = (row.get(\"article_title\",\"\") or \"\").strip().lower()\n",
    "    u = (row.get(\"article_url\",\"\") or \"\").strip().lower()\n",
    "    s = (row.get(\"article_snippet\",\"\") or \"\").strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)[:250]\n",
    "    return _hash16(f\"{t}||{u}||{s}\")\n",
    "\n",
    "if len(raw_hits) == 0:\n",
    "    dedup = raw_hits.copy()\n",
    "    dedup.to_csv(DEDUP_OUT, index=False)\n",
    "    print(\"[OK] wrote empty dedup file:\", DEDUP_OUT)\n",
    "else:\n",
    "    raw_hits[\"article_fingerprint\"] = raw_hits.apply(_fp, axis=1)\n",
    "\n",
    "    # Prefer earlier pub date per deal\n",
    "    # (publication_date exists now; if missing, it sorts last)\n",
    "    raw_hits = raw_hits.sort_values([\"deal_id\",\"publication_date\"], na_position=\"last\")\n",
    "\n",
    "    before = len(raw_hits)\n",
    "    dedup = raw_hits.drop_duplicates([\"deal_id\",\"article_fingerprint\"], keep=\"first\").copy()\n",
    "    after = len(dedup)\n",
    "\n",
    "    dedup.to_csv(DEDUP_OUT, index=False)\n",
    "    print(f\"[OK] dedup: {before:,} -> {after:,} rows | wrote -> {DEDUP_OUT}\")\n",
    "\n",
    "    # diagnostics: how many deals got any hits?\n",
    "    n_deals = int(dedup[\"deal_id\"].nunique())\n",
    "    print(\"[diag] unique deals with hits:\", n_deals)\n",
    "    if n_deals > 0:\n",
    "        hits_per_deal = dedup.groupby(\"deal_id\").size()\n",
    "        print(\"[diag] hits/deal: mean=\", float(hits_per_deal.mean()), \"median=\", float(hits_per_deal.median()), \"max=\", int(hits_per_deal.max()))\n",
    "        print(\"[diag] top 10 deals by hits:\\n\", hits_per_deal.sort_values(ascending=False).head(10))\n",
    "\n",
    "print(f\"[time] step4: {time.time()-t4:.1f}s\")\n",
    "print(\"==============================================\\n\")\n",
    "\n",
    "# -----------------------\n",
    "# STEP 5 — CREATE MANUAL EVIDENCE TEMPLATE\n",
    "# -----------------------\n",
    "print(\"========== STEP 5: EVIDENCE TEMPLATE ==========\")\n",
    "\n",
    "tmpl_cols = [\n",
    "    \"deal_id\",\n",
    "    \"news_source\",\n",
    "    \"article_title\",\n",
    "    \"publication_date\",\n",
    "    \"article_url\",\n",
    "    \"price_disclosed\",      # 0/1\n",
    "    \"price_value\",          # numeric or NA\n",
    "    \"currency\",             # USD/EUR/...\n",
    "    \"price_sentence\",       # verbatim sentence containing the number\n",
    "    \"price_context\",        # +/- 1 sentence\n",
    "    \"undisclosed_flag\",     # 1 if 'terms not disclosed'\n",
    "    \"notes\",\n",
    "]\n",
    "pd.DataFrame(columns=tmpl_cols).to_csv(EVIDENCE_TEMPLATE_OUT, index=False)\n",
    "\n",
    "print(f\"[OK] wrote -> {EVIDENCE_TEMPLATE_OUT}\")\n",
    "print(\"\\nNEXT YOU DO MANUALLY:\")\n",
    "print(\"1) Open:\", DEDUP_OUT)\n",
    "print(\"2) Read each article_url\")\n",
    "print(\"3) Fill one row per validated article-deal match into:\", EVIDENCE_TEMPLATE_OUT)\n",
    "print(\"=============================================\\n\")\n",
    "\n",
    "print(\"DONE. Files present in newsdata/:\")\n",
    "print(sorted([p.name for p in OUTDIR.glob(\"*\")]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ba925f8-9fd5-4402-a8f6-29e4801ad8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[resume] existing hits: 282 | deals done: 19\n",
      "[info] remaining deals: 1318 | workers: 4\n",
      "\n",
      "[diag] testing first 30 remaining deals to see failure type...\n",
      "[diag] test errors: 0 / 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea855cb3826d4cdeb7aa23b3eba96849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RSS safe-parallel:   0%|          | 0/1318 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[checkpoint] processed=200 | total_hits=287 | nonempty_deals=5 | errors=0 | elapsed=11.2s\n",
      "[checkpoint] processed=400 | total_hits=290 | nonempty_deals=8 | errors=0 | elapsed=23.4s\n",
      "[checkpoint] processed=600 | total_hits=300 | nonempty_deals=12 | errors=0 | elapsed=36.0s\n",
      "[checkpoint] processed=800 | total_hits=300 | nonempty_deals=12 | errors=112 | elapsed=1014.8s\n",
      "[checkpoint] processed=1,000 | total_hits=300 | nonempty_deals=12 | errors=312 | elapsed=2692.8s\n",
      "[checkpoint] processed=1,200 | total_hits=300 | nonempty_deals=12 | errors=512 | elapsed=4370.7s\n",
      "\n",
      "[DONE]\n",
      "total_hits: 300 | unique deals w/ hits: 31\n",
      "nonempty deals (this run): 12 | errors (this run): 630\n",
      "raw hits file: newsdata/raw_news_hits.csv\n",
      "error log file: newsdata/rss_errors.csv\n",
      "time (s): 5391.5\n"
     ]
    }
   ],
   "source": [
    "# GOOGLE NEWS RSS — SAFE PARALLEL (LOW WORKERS) + RETRIES + ERROR LOG\n",
    "# This replaces your parallel downloader. It keeps your existing RAW_HITS_OUT and resumes.\n",
    "\n",
    "import requests, time, random\n",
    "import xml.etree.ElementTree as ET\n",
    "from urllib.parse import quote_plus\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "OUTDIR = Path(\"newsdata\")\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RAW_HITS_OUT = OUTDIR / \"raw_news_hits.csv\"\n",
    "ERROR_LOG_OUT = OUTDIR / \"rss_errors.csv\"\n",
    "\n",
    "MAX_WORKERS = 4          # <-- keep low (2–5). 30 triggers blocking.\n",
    "TIMEOUT_S = 20\n",
    "RETRIES = 4              # retry attempts per request\n",
    "BASE_SLEEP = 0.2         # base backoff\n",
    "JITTER = 0.2             # random jitter added\n",
    "\n",
    "WINDOW_DAYS = 60\n",
    "CHECKPOINT_EVERY = 200\n",
    "\n",
    "HIT_COLS = [\n",
    "    \"deal_id\",\"query_text\",\"date_start\",\"date_end\",\n",
    "    \"buyer_name\",\"seller_name\",\"seller_cik\",\"buyer_cik\",\"n_patents\",\n",
    "    \"article_title\",\"article_snippet\",\"news_source\",\"publication_date_raw\",\"article_url\"\n",
    "]\n",
    "\n",
    "def _rss_url(q: str) -> str:\n",
    "    base = \"https://news.google.com/rss/search\"\n",
    "    params = f\"q={quote_plus(q)}&hl=en-US&gl=US&ceid=US:en\"\n",
    "    return f\"{base}?{params}\"\n",
    "\n",
    "def _parse_rss(xml_text: str):\n",
    "    root = ET.fromstring(xml_text)\n",
    "    out = []\n",
    "    for item in root.findall(\".//item\"):\n",
    "        title = (item.findtext(\"title\") or \"\").strip()\n",
    "        link = (item.findtext(\"link\") or \"\").strip()\n",
    "        pubDate = (item.findtext(\"pubDate\") or \"\").strip()\n",
    "        source_el = item.find(\"source\")\n",
    "        source = (source_el.text or \"\").strip() if source_el is not None else \"\"\n",
    "        desc = (item.findtext(\"description\") or \"\").strip()\n",
    "        out.append((title, desc, source, pubDate, link))\n",
    "    return out\n",
    "\n",
    "# headers help a lot\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari\",\n",
    "    \"Accept\": \"application/rss+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "\n",
    "# load deals and create ONE query per deal\n",
    "deals = pd.read_csv(\"deals_for_sec.csv\")\n",
    "deals[\"deal_date\"] = pd.to_datetime(deals[\"deal_date\"], errors=\"coerce\")\n",
    "deals[\"date_start\"] = (deals[\"deal_date\"] - pd.to_timedelta(WINDOW_DAYS, unit=\"D\")).dt.date.astype(str)\n",
    "deals[\"date_end\"]   = (deals[\"deal_date\"] + pd.to_timedelta(WINDOW_DAYS, unit=\"D\")).dt.date.astype(str)\n",
    "deals[\"query_text\"] = deals[\"ee_name\"].astype(str) + \" \" + deals[\"or_name\"].astype(str) + \" patent portfolio\"\n",
    "\n",
    "# resume raw hits\n",
    "if RAW_HITS_OUT.exists():\n",
    "    raw_hits = pd.read_csv(RAW_HITS_OUT)\n",
    "    for c in HIT_COLS:\n",
    "        if c not in raw_hits.columns:\n",
    "            raw_hits[c] = pd.NA\n",
    "    raw_hits = raw_hits[HIT_COLS]\n",
    "    done_deals = set(raw_hits[\"deal_id\"].dropna().astype(int).unique().tolist())\n",
    "    print(\"[resume] existing hits:\", len(raw_hits), \"| deals done:\", len(done_deals))\n",
    "else:\n",
    "    raw_hits = pd.DataFrame(columns=HIT_COLS)\n",
    "    done_deals = set()\n",
    "    raw_hits.to_csv(RAW_HITS_OUT, index=False)\n",
    "    print(\"[init] created\", RAW_HITS_OUT)\n",
    "\n",
    "# resume error log (optional)\n",
    "if ERROR_LOG_OUT.exists():\n",
    "    err_log = pd.read_csv(ERROR_LOG_OUT)\n",
    "else:\n",
    "    err_log = pd.DataFrame(columns=[\"deal_id\",\"url\",\"status\",\"error\"])\n",
    "\n",
    "todo = [r for r in deals.to_dict(\"records\") if int(r[\"deal_id\"]) not in done_deals]\n",
    "print(\"[info] remaining deals:\", len(todo), \"| workers:\", MAX_WORKERS)\n",
    "\n",
    "def fetch_one(rowdict):\n",
    "    \"\"\"\n",
    "    Returns: (deal_id, q, items, status_code, error_str)\n",
    "    Retries with exponential backoff on any failure.\n",
    "    \"\"\"\n",
    "    q = f'{rowdict[\"query_text\"]} after:{rowdict[\"date_start\"]} before:{rowdict[\"date_end\"]}'\n",
    "    url = _rss_url(q)\n",
    "\n",
    "    last_err = None\n",
    "    last_status = None\n",
    "\n",
    "    for a in range(RETRIES):\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=TIMEOUT_S)\n",
    "            last_status = r.status_code\n",
    "\n",
    "            # Hard block / rate limit -> backoff\n",
    "            if r.status_code in (429, 503, 502, 500):\n",
    "                last_err = f\"HTTP {r.status_code}\"\n",
    "                time.sleep((BASE_SLEEP * (2 ** a)) + random.random() * JITTER)\n",
    "                continue\n",
    "\n",
    "            r.raise_for_status()\n",
    "            items = _parse_rss(r.text)\n",
    "            return rowdict[\"deal_id\"], q, items, r.status_code, None\n",
    "\n",
    "        except Exception as e:\n",
    "            last_err = repr(e)\n",
    "            time.sleep((BASE_SLEEP * (2 ** a)) + random.random() * JITTER)\n",
    "\n",
    "    return rowdict[\"deal_id\"], q, [], last_status, last_err\n",
    "\n",
    "# ---- quick diagnosis on 30 deals (prints sample errors)\n",
    "print(\"\\n[diag] testing first 30 remaining deals to see failure type...\")\n",
    "test = todo[:30]\n",
    "test_errors = []\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futs = [ex.submit(fetch_one, r) for r in test]\n",
    "    for fut in as_completed(futs):\n",
    "        deal_id, q, items, status, err = fut.result()\n",
    "        if err:\n",
    "            test_errors.append((deal_id, status, err))\n",
    "print(\"[diag] test errors:\", len(test_errors), \"/ 30\")\n",
    "for x in test_errors[:10]:\n",
    "    print(\"  deal_id=\", x[0], \"| status=\", x[1], \"| err=\", x[2])\n",
    "\n",
    "# ---- full run\n",
    "t0 = time.time()\n",
    "new_rows = []\n",
    "processed = 0\n",
    "n_nonempty = 0\n",
    "n_errors = 0\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futs = {ex.submit(fetch_one, r): r for r in todo}\n",
    "    for fut in tqdm(as_completed(futs), total=len(futs), desc=\"RSS safe-parallel\"):\n",
    "        r0 = futs[fut]\n",
    "        deal_id, q, items, status, err = fut.result()\n",
    "        processed += 1\n",
    "\n",
    "        if err:\n",
    "            n_errors += 1\n",
    "            err_log = pd.concat([err_log, pd.DataFrame([{\n",
    "                \"deal_id\": deal_id,\n",
    "                \"url\": _rss_url(q),\n",
    "                \"status\": status,\n",
    "                \"error\": err\n",
    "            }])], ignore_index=True)\n",
    "\n",
    "        if len(items) > 0:\n",
    "            n_nonempty += 1\n",
    "\n",
    "        for title, desc, source, pubDate, link in items:\n",
    "            new_rows.append({\n",
    "                \"deal_id\": r0[\"deal_id\"],\n",
    "                \"query_text\": q,\n",
    "                \"date_start\": r0[\"date_start\"],\n",
    "                \"date_end\": r0[\"date_end\"],\n",
    "                \"buyer_name\": r0[\"ee_name\"],\n",
    "                \"seller_name\": r0[\"or_name\"],\n",
    "                \"seller_cik\": r0[\"seller_cik\"],\n",
    "                \"buyer_cik\": r0[\"buyer_cik\"],\n",
    "                \"n_patents\": r0[\"n_patents\"],\n",
    "                \"article_title\": title,\n",
    "                \"article_snippet\": desc,\n",
    "                \"news_source\": source,\n",
    "                \"publication_date_raw\": pubDate,\n",
    "                \"article_url\": link,\n",
    "            })\n",
    "\n",
    "        if processed % CHECKPOINT_EVERY == 0:\n",
    "            if len(new_rows) > 0:\n",
    "                raw_hits = pd.concat([raw_hits, pd.DataFrame(new_rows, columns=HIT_COLS)], ignore_index=True)\n",
    "                new_rows = []\n",
    "            raw_hits.to_csv(RAW_HITS_OUT, index=False)\n",
    "            err_log.to_csv(ERROR_LOG_OUT, index=False)\n",
    "\n",
    "            print(f\"[checkpoint] processed={processed:,} | total_hits={len(raw_hits):,} | nonempty_deals={n_nonempty:,} \"\n",
    "                  f\"| errors={n_errors:,} | elapsed={time.time()-t0:.1f}s\")\n",
    "\n",
    "# final flush\n",
    "if len(new_rows) > 0:\n",
    "    raw_hits = pd.concat([raw_hits, pd.DataFrame(new_rows, columns=HIT_COLS)], ignore_index=True)\n",
    "raw_hits.to_csv(RAW_HITS_OUT, index=False)\n",
    "err_log.to_csv(ERROR_LOG_OUT, index=False)\n",
    "\n",
    "print(\"\\n[DONE]\")\n",
    "print(\"total_hits:\", len(raw_hits), \"| unique deals w/ hits:\", raw_hits[\"deal_id\"].nunique())\n",
    "print(\"nonempty deals (this run):\", n_nonempty, \"| errors (this run):\", n_errors)\n",
    "print(\"raw hits file:\", RAW_HITS_OUT)\n",
    "print(\"error log file:\", ERROR_LOG_OUT)\n",
    "print(\"time (s):\", round(time.time()-t0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "290898c2-137c-49b0-989b-1f338a2d96e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors rows: 630\n",
      "status counts:\n",
      " status\n",
      "503    630\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top error strings:\n",
      " error\n",
      "HTTP 503    630\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Inspect rss_errors.csv\n",
    "err = pd.read_csv(\"newsdata/rss_errors.csv\")\n",
    "print(\"errors rows:\", len(err))\n",
    "print(\"status counts:\\n\", err[\"status\"].value_counts(dropna=False).head(20))\n",
    "print(\"\\nTop error strings:\\n\", err[\"error\"].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8a08292-2cee-4501-ba1a-1c7eb5b2dfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of deals with any news: 19\n",
      "Deal IDs: [ 0  1  7 12 13 14 15 16 22 23 37 39 40 42 43 44 45 46 49]\n",
      "(19, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>or_name</th>\n",
       "      <th>ee_name</th>\n",
       "      <th>deal_date</th>\n",
       "      <th>n_patents</th>\n",
       "      <th>seller_cik</th>\n",
       "      <th>buyer_cik</th>\n",
       "      <th>deal_id</th>\n",
       "      <th>filer_cik</th>\n",
       "      <th>sec_start</th>\n",
       "      <th>sec_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A9COM INC</td>\n",
       "      <td>AMAZON TECHNOLOGIES INC</td>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1321834.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1321834.0</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>2020-07-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBOTT LABORATORIES</td>\n",
       "      <td>ABBOTT DIABETES CARE INC</td>\n",
       "      <td>2014-08-18</td>\n",
       "      <td>36</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>2014-05-20</td>\n",
       "      <td>2014-11-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AERAS</td>\n",
       "      <td>INTERNATIONAL AIDS VACCINE INITIATIVE INC</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105319.0</td>\n",
       "      <td>7</td>\n",
       "      <td>105319.0</td>\n",
       "      <td>2018-07-03</td>\n",
       "      <td>2018-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AHN JOONKUI</td>\n",
       "      <td>LG ELECTRONICS INC</td>\n",
       "      <td>2013-02-28</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2058873.0</td>\n",
       "      <td>12</td>\n",
       "      <td>2058873.0</td>\n",
       "      <td>2012-11-30</td>\n",
       "      <td>2013-05-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AIR PRODUCTS AND CHEMICALS INC</td>\n",
       "      <td>EVONIK DEGUSSA GMBH</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>47</td>\n",
       "      <td>2969.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>2969.0</td>\n",
       "      <td>2016-10-05</td>\n",
       "      <td>2017-04-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          or_name                                    ee_name  \\\n",
       "0                       A9COM INC                    AMAZON TECHNOLOGIES INC   \n",
       "1             ABBOTT LABORATORIES                   ABBOTT DIABETES CARE INC   \n",
       "2                           AERAS  INTERNATIONAL AIDS VACCINE INITIATIVE INC   \n",
       "3                     AHN JOONKUI                         LG ELECTRONICS INC   \n",
       "4  AIR PRODUCTS AND CHEMICALS INC                        EVONIK DEGUSSA GMBH   \n",
       "\n",
       "    deal_date  n_patents  seller_cik  buyer_cik  deal_id  filer_cik  \\\n",
       "0  2020-04-22         30         NaN  1321834.0        0  1321834.0   \n",
       "1  2014-08-18         36      1800.0        NaN        1     1800.0   \n",
       "2  2018-10-01         22         NaN   105319.0        7   105319.0   \n",
       "3  2013-02-28         27         NaN  2058873.0       12  2058873.0   \n",
       "4  2017-01-03         47      2969.0        NaN       13     2969.0   \n",
       "\n",
       "    sec_start     sec_end  \n",
       "0  2020-01-23  2020-07-21  \n",
       "1  2014-05-20  2014-11-16  \n",
       "2  2018-07-03  2018-12-30  \n",
       "3  2012-11-30  2013-05-29  \n",
       "4  2016-10-05  2017-04-03  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dedup = pd.read_csv(\"newsdata/news_hits_deduped.csv\")\n",
    "\n",
    "# list the deals that have any news\n",
    "deal_ids_with_news = dedup[\"deal_id\"].unique()\n",
    "\n",
    "print(\"Number of deals with any news:\", len(deal_ids_with_news))\n",
    "print(\"Deal IDs:\", deal_ids_with_news)\n",
    "deals = pd.read_csv(\"deals_for_sec.csv\")\n",
    "\n",
    "deals_with_news = deals.merge(\n",
    "    pd.DataFrame({\"deal_id\": deal_ids_with_news}),\n",
    "    on=\"deal_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(deals_with_news.shape)\n",
    "deals_with_news.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a8046-8a2c-4b1f-a02f-291bdbba6d57",
   "metadata": {},
   "source": [
    "# Lets investigate one of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28577173-dc30-4f07-b761-91b4997793d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles for deal 0: 21\n"
     ]
    }
   ],
   "source": [
    "d0 = dedup[dedup[\"deal_id\"] == 0]\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "print(\"Number of articles for deal 0:\", len(d0))\n",
    "#print(d0['article_snippet'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97242eb4-4150-4764-b602-f07ed8cfbb25",
   "metadata": {},
   "source": [
    "# News route (Google News, free) — what we did, what happened, and current outputs\n",
    "\n",
    "## Goal\n",
    "Starting from the fixed universe of **1,337 USPTO-identified patent transfers** in `deals_for_sec.csv`, collect **news coverage evidence** (and any **explicitly disclosed prices**) using **Google News (free)**.  \n",
    "This is a **verification/disclosure layer**, not deal discovery.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs (fixed)\n",
    "**File**\n",
    "- `deals_for_sec.csv` (1,337 rows)\n",
    "\n",
    "**Required columns used**\n",
    "- `deal_id`, `or_name` (seller), `ee_name` (buyer), `deal_date`, `n_patents`, `seller_cik`, `buyer_cik`\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-step: what we implemented\n",
    "\n",
    "### Step 1 — Generate deal-level search queries\n",
    "We created a query file from the 1,337 deals, using a ±60-day window around each `deal_date`.\n",
    "\n",
    "**Output**\n",
    "- `newsdata/news_queries.csv`\n",
    "- Size: **5,348 queries** (= 1,337 deals × 4 query templates)\n",
    "\n",
    "Example query templates used:\n",
    "- `\"{buyer} patents acquisition\"`\n",
    "- `\"{buyer} acquired patents from {seller}\"`\n",
    "- `\"{seller} patent portfolio sale\"`\n",
    "- `\"{buyer} \"intellectual property\" purchase\"`\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2 — Attempt #1: Python `gnews` library (failed / returned 0 results)\n",
    "We tried the `gnews` Python library to pull Google News hits programmatically.\n",
    "\n",
    "Observed behavior:\n",
    "- Queries ran, but returned **zero results for all queries** (`nonempty_queries=0`, `errors=0`).\n",
    "- This created an empty hits dataframe and originally triggered a `KeyError` (because the hits list was empty and the DataFrame had no columns).\n",
    "\n",
    "Fix:\n",
    "- We changed logic to **always enforce a schema** and to **checkpoint-save** so empty results never crash.\n",
    "\n",
    "Conclusion:\n",
    "- `gnews` was not usable in this environment (likely scraping endpoint changes / blocked).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3 — Attempt #2: Google News RSS endpoint (worked initially)\n",
    "We switched to the **Google News RSS search endpoint** (free, no API key).  \n",
    "We embedded the date window directly into the query text using operators:\n",
    "\n",
    "- `after:YYYY-MM-DD before:YYYY-MM-DD`\n",
    "\n",
    "We ran sequential batches successfully:\n",
    "- First small batches yielded hits and saved:\n",
    "  - `newsdata/raw_news_hits.csv`\n",
    "\n",
    "At that stage we got:\n",
    "- window-filtered raw hits around **~282** (later stabilized at **282 rows** after cleaning)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4 — Clean: parse publication dates + enforce date window\n",
    "We parsed:\n",
    "- `publication_date_raw` → `publication_date`\n",
    "- `date_start`, `date_end` → datetime\n",
    "\n",
    "Then kept only articles where:\n",
    "- publication_date is within `[date_start, date_end]`\n",
    "\n",
    "Result after filtering:\n",
    "- `newsdata/raw_news_hits.csv` had **282 rows**\n",
    "- publication_date missing share was **0.0** (so window filter was fully enforceable)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5 — De-duplicate within deal\n",
    "We created a fingerprint:\n",
    "- hash(title + url + snippet[:250])\n",
    "\n",
    "Then dropped duplicates within each `deal_id`.\n",
    "\n",
    "**Output**\n",
    "- `newsdata/news_hits_deduped.csv`\n",
    "\n",
    "Current counts after dedup:\n",
    "- **265 article–deal rows**\n",
    "- **19 unique deals** with any news coverage (based on unique `deal_id`)\n",
    "\n",
    "The 19 deals with coverage:\n",
    "- `[0, 1, 7, 12, 13, 14, 15, 16, 22, 23, 37, 39, 40, 42, 43, 44, 45, 46, 49]`\n",
    "\n",
    "Interpretation:\n",
    "- Only ~**1.4%** of the 1,337 patent transfers appear in Google News at all (19/1337).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6 — Create manual evidence template (for price extraction)\n",
    "We created a template for manual price validation (because price extraction requires reading the article and recording the exact sentence).\n",
    "\n",
    "**Output**\n",
    "- `newsdata/news_price_evidence_template.csv`\n",
    "\n",
    "Fields include:\n",
    "- deal_id, source, title, date, url\n",
    "- price_disclosed (0/1), price_value, currency\n",
    "- price_sentence (verbatim), price_context, undisclosed_flag, notes\n",
    "\n",
    "---\n",
    "\n",
    "## What happened when we tried to scale up to all 1,337 deals\n",
    "We tried speeding up Step 3 (RSS pulls) via parallel requests.\n",
    "\n",
    "### Parallel run #1 (30 workers)\n",
    "Result:\n",
    "- **Immediate blocking**: essentially all requests failed.\n",
    "- Errors: 1,318 / 1,318\n",
    "- Hits did not increase.\n",
    "\n",
    "### Parallel run #2 (“safe” 4 workers + retries)\n",
    "Result:\n",
    "- Worked at first (hits increased modestly), then began failing.\n",
    "- Eventually Google returned **HTTP 503** at scale.\n",
    "\n",
    "We inspected:\n",
    "- `newsdata/rss_errors.csv`\n",
    "- 630 errors, **all HTTP 503**\n",
    "\n",
    "Meaning:\n",
    "- Google News RSS began **throttling/soft-blocking** after sustained automated querying.\n",
    "- After the block started, additional requests produced no new data (hits stopped increasing).\n",
    "\n",
    "---\n",
    "\n",
    "## Current state (what you have now)\n",
    "**Files produced**\n",
    "- `newsdata/news_queries.csv` — 5,348 query rows (4 queries per deal)\n",
    "- `newsdata/raw_news_hits.csv` — raw RSS hits after windowing (currently ~282 rows)\n",
    "- `newsdata/news_hits_deduped.csv` — deduped hits (currently **265 rows**)\n",
    "- `newsdata/news_price_evidence_template.csv` — manual extraction template\n",
    "- `newsdata/rss_errors.csv` — error log (shows HTTP 503 blocking)\n",
    "\n",
    "**Coverage result so far**\n",
    "- **19 deals** have any Google News coverage in the deduped file.\n",
    "\n",
    "---\n",
    "\n",
    "## Why we stop here (for now)\n",
    "- Google started returning **HTTP 503** in bulk (blocking).\n",
    "- Continuing automated crawling risks wasting time and possibly increasing block duration.\n",
    "- Empirically, the yield is already small and consistent with “news coverage is rare.”\n",
    "\n",
    "---\n",
    "\n",
    "## What can be done later (if needed)\n",
    "If you want to push further later, you can:\n",
    "- Run in **small sessions** (e.g., 100–200 deals per session)\n",
    "- Use a **global rate cap** (e.g., 1 request per 4–5 seconds total)\n",
    "- Avoid high concurrency\n",
    "But expect diminishing returns (most deals will still have no coverage).\n",
    "\n",
    "---\n",
    "\n",
    "## Next step (actionable, no more scraping)\n",
    "For the **19 covered deals**:\n",
    "1. Open `newsdata/news_hits_deduped.csv`\n",
    "2. Click each `article_url`\n",
    "3. Confirm if it is truly a patent/IP transfer and whether price is disclosed\n",
    "4. Record validated evidence in `newsdata/news_price_evidence_template.csv`\n",
    "\n",
    "This yields a small, high-credibility price/disclosure sample suitable for a “disclosure outcome” design.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ed999f-1e7f-4e3b-843b-13a78032ecaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
